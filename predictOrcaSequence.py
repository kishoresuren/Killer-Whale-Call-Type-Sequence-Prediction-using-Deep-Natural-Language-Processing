# -*- coding: utf-8 -*-
"""
Created on Mon May  6 20:02:44 2019

@author: Kishore
"""
import numpy as np
import os
import pickle
from keras.models import load_model
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import argparse

def fetchData():
    return [np.load('XTrainSet.npy') , np.load('XTestSet.npy') ,np.load('YTrainSet.npy'), np.load('YTestSet.npy')]


def tokenize(list,tokenizer):
    tokenizer.fit_on_texts(list)
    temp = tokenizer.texts_to_sequences(list)
    temp1=[]
    for arr in temp:
        temp1.append([val-1 for val in arr])
    return temp1

def generate_text(model, tokenizer, seq_len, text_sequences, num_gen_words):
    '''
    INPUTS:
    model : model that was trained on text data
    tokenizer : tokenizer that was fit on text data
    seq_len : length of training sequence
    seed_text : raw string text to serve as the seed
    num_gen_words : number of words to be generated by model
    '''
    
    all_text=[]
    output_text = []
    seedList = []

    
    # Create num_gen_words
    for index in range(len(text_sequences)):
        if index >= 1:
            seedList.append(seed_text)
            all_text.append(output_text) 
            output_text =[] 
        seed_text = ' '.join(str(text_sequences[index]))  
        for i in range(num_gen_words):   
        # Take the input text string and encode it to a sequence
            encoded_text = tokenizer.texts_to_sequences([seed_text])[0]

        
        # Pad sequences to our trained rate (Equal to window width (Default:5))
            pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')
            pad_encoded = pad_encoded.reshape(1,seq_len,1)
        
        # Predict Class Probabilities for each word
            pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]

        
        # Grab word
            pred_word = tokenizer.index_word[pred_word_ind+1] 
        
        # Update the sequence of input text (shifting one over with the new word)
            seed_text += ' ' + pred_word
        
            output_text.append(pred_word)
        
    # Make it look like a sequence.
    return [all_text,seedList]

parser = argparse.ArgumentParser(description='Predicting an orca sequence')
parser.add_argument('--modelType', default='Bi',required=False,
                    help='Can be Uni for unidirectional model or Bi for bidirectional model')
parser.add_argument('--modelName', default='LSTM',required=False,
                    help='Can be LSTM or GRU')
parser.add_argument('--numGenWords', default='3',required=False,
                    help='Can be any valid integer. Suggested to be less than the window width')

args = parser.parse_args()

#Load the previously saved tokenizer
with open('orcaTokenizer', 'rb') as handle:
    tokenizer = pickle.load(handle)


testData = np.load('XTestSet.npy').tolist()
    
sequence = tokenize(testData,tokenizer)
XTest = np.array(sequence)
yTest = to_categorical(XTest[:,-1])


#Load model
if not os.path.isfile(args.modelType+args.modelName+'.h5'):
    print('Model '+args.modelType+args.modelName+'.h5'+' has not yet been created. Please run buildModel.py for the same.')
else:
    model = load_model(args.modelType+args.modelName+'.h5')

    XTest = XTest.reshape(XTest.shape[0],XTest.shape[1],1)
    num_gen_words = int(args.numGenWords)
    y_guess , y_pred = generate_text(model, tokenizer, XTest.shape[1], XTest, num_gen_words=num_gen_words)


    for i in range(len(y_guess)):
        print( str(testData[i]) + '>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>' + str(y_guess[i]) )

    np.save('YGuess.npy',y_guess)